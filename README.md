# MoralStoriesContrastSet
Abstract: 
Abstract
Predictive ethics models like Delphi have achieved unprecedented accuracy in recent years, spiking interest in the field. This paper challenges these models’ accuracy by identifying issues with translating moral dilemmas into text-based input. It demonstrates these issues with contrast sets that substantially reduces classifier performance on the dataset Moral Stories. Specifically, label-changing tweaks to a situation’s descriptive content (as small as 3-5 words) can reduce classifier accuracy to as low as 51%, almost half the initial accuracy of 99.9%. Associating situations with a misleading social norm lowers accuracy to 98.8%, while adding textual bias (i.e. an implication that a situation already fits a certain label) lowers accuracy to 77%. These results suggest not only that many ethics models have substantially overfit, but that several precautions are required to ensure that input accurately captures a moral dilemma. This paper recommends re-examining the structure of a social norm, training models to ask for context with defeasible reasoning, filtering input for textual bias, and recognizing culture’s role as the final moral arbiter of an action.
